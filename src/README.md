# ğŸ“Œ SQL Evaluator

Este repositorio es un **template** que los estudiantes pueden usar para evaluar consultas SQL automÃ¡ticamente mediante GitHub Actions. Sigue los pasos a continuaciÃ³n para configurar tu repositorio y comenzar con la evaluaciÃ³n.

---

## ğŸš€ 1. Configurar tu repositorio

1. **Usa este repositorio como plantilla**  
   - Ve a la pÃ¡gina de este repositorio en GitHub.  
   - Haz clic en **"Use this template"** y crea tu propio repositorio.  
   - Elige un nombre para tu repositorio y crÃ©alo.

2. **Clona tu repositorio**  
   ```sh
   git clone https://github.com/TU-USUARIO/TU-REPO.git
   cd TU-REPO
   ```

---

## âœï¸ 2. Edita el archivo de queries

- Modifica el archivo `queries/queries.sql` con las consultas SQL que desees evaluar.

---

## ğŸ“¤ 3. Sube tus cambios

   ```sh
   git add queries/queries.sql
   git commit -m "ğŸ’¾ AÃ±adir consultas SQL"
   git push origin main
   ```

---

## ğŸ› ï¸ 4. RevisiÃ³n automÃ¡tica con GitHub Actions

Cada vez que hagas un *push*, se ejecutarÃ¡ una acciÃ³n en **GitHub Actions** que evaluarÃ¡ tus consultas SQL.  
Para ver los resultados:

1. **Ve a la pestaÃ±a "Actions" en GitHub**.  
2. Abre el Ãºltimo *workflow run* para ver los detalles.  
3. Puedes descargar el reporte generado en **Artifacts** bajo el nombre `Reporte_SQL`.
4. Se va generer un archivo `RESULTADOS.md` en la raÃ­z de tu repositorio, que contendrÃ¡ el resultado de la evaluaciÃ³n.

---

## ğŸ” 5. Revisar el reporte

El archivo `RESULTADOS.md` contendrÃ¡ la evaluaciÃ³n de tus consultas. Puedes descargarlo desde **GitHub Actions** o revisarlo directamente en la raÃ­z del repositorio.

### ğŸ“¥ Obtener el reporte actualizado
Si `RESULTADOS.md` no aparece en tu copia local, ejecuta:

```sh
git pull origin main
```

Si necesitas corregir errores, actualiza `queries/queries.sql`, sube los cambios y vuelve a revisar el reporte.

---

## â“ Preguntas Frecuentes

### ğŸ“Œ Â¿QuÃ© evalÃºa este repositorio?
Este sistema revisa consultas SQL para verificar si los resultados coinciden con las salidas esperadas y si estÃ¡n optimizadas en tÃ©rminos de rendimiento.

### ğŸ›  Â¿CÃ³mo puedo ejecutar las pruebas en local?

Puedes ejecutar las pruebas en tu mÃ¡quina local con los siguientes pasos:

1. AsegÃºrate de tener Docker, Python 3 y `pip` instalados.
2. Levanta la base de datos con Docker Compose:

```sh
docker compose up -d
```

3. Activa tu entorno virtual de Python (si no lo tienes creado, haz primero `python3 -m venv venv`):

```sh
source venv/bin/activate
pip install -r requirements.txt
````

4. Ejecuta el script de evaluaciÃ³n:

```sh
python -m src.scripts.check_queries
```

Esto generarÃ¡ el archivo `RESULTADOS.md` con el anÃ¡lisis de tus consultas.


### ğŸ† Â¿CÃ³mo saber si mis consultas son correctas?
Si el reporte `RESULTADOS.md` indica que todas las queries son correctas (`âœ…`), significa que tus consultas cumplen con los criterios esperados.

Si hay errores (`âŒ`), revisa el reporte y ajusta tu cÃ³digo SQL en `queries/queries.sql`.

---

### ğŸ›  Â¿CÃ³mo puedo crear un evaluador de nuevas queries ?

1. AsegÃºrate de tener Docker, Python 3 y `pip` instalados.
2. AÃ±ade tu nuevo schema y aÃ±ade datos en el archivo init_db.sql 
3. Levanta la base de datos con Docker Compose:

```sh
docker compose up -d
```

4. Activa tu entorno virtual de Python (si no lo tienes creado, haz primero `python3 -m venv venv`):

```sh
source venv/bin/activate
pip install -r requirements.txt
````

5. Ejecuta el script para generar los archivos en la carpeta src/expected_results:

```sh
python -m src.scripts.generate_expected
```